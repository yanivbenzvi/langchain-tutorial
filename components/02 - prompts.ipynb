{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0b25a4a22a0b6a6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Prompts - Text generally used as instructions to your model\n",
    "### **Prompt**\n",
    "What you'll pass to the underlying model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81bbd69ce501703a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[llm:Ollama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Today is Monday, tomorrow is Wednesday.\\n\\nWhat is wrong with that statement?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[llm:Ollama] [1.73s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"A classic lateral thinking puzzle!\\n\\nThe answer is: \\\"Tomorrow can't be Wednesday if today is Monday.\\\"\\n\\nIn our normal understanding of the calendar, today being a Monday means that tomorrow would be Tuesday, not Wednesday. So, there's a logical inconsistency in the statement. Well done on crafting a clever puzzle!\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-07-15T13:23:38.289429Z\",\n",
      "          \"response\": \"\",\n",
      "          \"done\": true,\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"context\": [\n",
      "            128006,\n",
      "            882,\n",
      "            128007,\n",
      "            1432,\n",
      "            15724,\n",
      "            374,\n",
      "            7159,\n",
      "            11,\n",
      "            16986,\n",
      "            374,\n",
      "            8079,\n",
      "            382,\n",
      "            3923,\n",
      "            374,\n",
      "            5076,\n",
      "            449,\n",
      "            430,\n",
      "            5224,\n",
      "            5380,\n",
      "            128009,\n",
      "            128006,\n",
      "            78191,\n",
      "            128007,\n",
      "            271,\n",
      "            32,\n",
      "            11670,\n",
      "            45569,\n",
      "            7422,\n",
      "            25649,\n",
      "            2268,\n",
      "            791,\n",
      "            4320,\n",
      "            374,\n",
      "            25,\n",
      "            330,\n",
      "            91273,\n",
      "            649,\n",
      "            956,\n",
      "            387,\n",
      "            8079,\n",
      "            422,\n",
      "            3432,\n",
      "            374,\n",
      "            7159,\n",
      "            2266,\n",
      "            644,\n",
      "            1057,\n",
      "            4725,\n",
      "            8830,\n",
      "            315,\n",
      "            279,\n",
      "            13470,\n",
      "            11,\n",
      "            3432,\n",
      "            1694,\n",
      "            264,\n",
      "            7159,\n",
      "            3445,\n",
      "            430,\n",
      "            16986,\n",
      "            1053,\n",
      "            387,\n",
      "            7742,\n",
      "            11,\n",
      "            539,\n",
      "            8079,\n",
      "            13,\n",
      "            2100,\n",
      "            11,\n",
      "            1070,\n",
      "            596,\n",
      "            264,\n",
      "            20406,\n",
      "            97249,\n",
      "            304,\n",
      "            279,\n",
      "            5224,\n",
      "            13,\n",
      "            8489,\n",
      "            2884,\n",
      "            389,\n",
      "            45167,\n",
      "            264,\n",
      "            28799,\n",
      "            25649,\n",
      "            0\n",
      "          ],\n",
      "          \"total_duration\": 1724972125,\n",
      "          \"load_duration\": 34130584,\n",
      "          \"prompt_eval_count\": 25,\n",
      "          \"prompt_eval_duration\": 480611000,\n",
      "          \"eval_count\": 63,\n",
      "          \"eval_duration\": 1209403000\n",
      "        },\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "A classic lateral thinking puzzle!\n",
      "\n",
      "The answer is: \"Tomorrow can't be Wednesday if today is Monday.\"\n",
      "\n",
      "In our normal understanding of the calendar, today being a Monday means that tomorrow would be Tuesday, not Wednesday. So, there's a logical inconsistency in the statement. Well done on crafting a clever puzzle!\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "import langchain\n",
    "langchain.debug = True\n",
    "\n",
    "llama3_llm = Ollama(model=\"llama3\")\n",
    "prompt = \"\"\"\n",
    "Today is Monday, tomorrow is Wednesday.\n",
    "\n",
    "What is wrong with that statement?\n",
    "\"\"\"\n",
    "\n",
    "print(llama3_llm.invoke(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f9a5839e40007c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### **Prompt Template**\n",
    "An object that helps create prompts based on a combination of user input, other non-static information and a fixed template string.\n",
    "\n",
    "Think of it as an [f-string](https://realpython.com/python-f-strings/) in python but for prompts\n",
    "\n",
    "*Advanced: Check out LangSmithHub(https://smith.langchain.com/hub) for many more community prompt templates*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e3dd481831c0fdf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Prompt: \n",
      "I really want to travel to Rome. What should I do there?\n",
      "\n",
      "Respond in one short sentence\n",
      "\n",
      "-----------\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[llm:Ollama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"I really want to travel to Rome. What should I do there?\\n\\nRespond in one short sentence\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[llm:Ollama] [960ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Explore the ancient ruins of the Colosseum, Roman Forum, and Pantheon, and indulge in delicious Italian cuisine and gelato throughout your stay.\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-07-15T13:25:44.974428Z\",\n",
      "          \"response\": \"\",\n",
      "          \"done\": true,\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"context\": [\n",
      "            128006,\n",
      "            882,\n",
      "            128007,\n",
      "            1432,\n",
      "            40,\n",
      "            2216,\n",
      "            1390,\n",
      "            311,\n",
      "            5944,\n",
      "            311,\n",
      "            22463,\n",
      "            13,\n",
      "            3639,\n",
      "            1288,\n",
      "            358,\n",
      "            656,\n",
      "            1070,\n",
      "            1980,\n",
      "            66454,\n",
      "            304,\n",
      "            832,\n",
      "            2875,\n",
      "            11914,\n",
      "            198,\n",
      "            128009,\n",
      "            128006,\n",
      "            78191,\n",
      "            128007,\n",
      "            271,\n",
      "            52361,\n",
      "            279,\n",
      "            14154,\n",
      "            46762,\n",
      "            315,\n",
      "            279,\n",
      "            4349,\n",
      "            437,\n",
      "            325,\n",
      "            372,\n",
      "            11,\n",
      "            13041,\n",
      "            17997,\n",
      "            11,\n",
      "            323,\n",
      "            11233,\n",
      "            64110,\n",
      "            11,\n",
      "            323,\n",
      "            68190,\n",
      "            304,\n",
      "            18406,\n",
      "            15155,\n",
      "            36105,\n",
      "            323,\n",
      "            18316,\n",
      "            4428,\n",
      "            6957,\n",
      "            701,\n",
      "            4822,\n",
      "            13\n",
      "          ],\n",
      "          \"total_duration\": 951661250,\n",
      "          \"load_duration\": 25508958,\n",
      "          \"prompt_eval_count\": 30,\n",
      "          \"prompt_eval_duration\": 310284000,\n",
      "          \"eval_count\": 32,\n",
      "          \"eval_duration\": 615072000\n",
      "        },\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "LLM Output: Explore the ancient ruins of the Colosseum, Roman Forum, and Pantheon, and indulge in delicious Italian cuisine and gelato throughout your stay.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "llm = Ollama(model=\"llama3\")\n",
    "\n",
    "# Notice \"location\" below, that is a placeholder for another value later\n",
    "template = \"\"\"\n",
    "I really want to travel to {location}. What should I do there?\n",
    "\n",
    "Respond in one short sentence\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"location\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "final_prompt = prompt.format(location='Rome')\n",
    "\n",
    "print (f\"Final Prompt: {final_prompt}\")\n",
    "print (\"-----------\")\n",
    "print (f\"LLM Output: {llm.invoke(final_prompt)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58c167143648d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### **Example Selectors**\n",
    "An easy way to select from a series of examples that allow you to dynamic place in-context information into your prompt. Often used when your task is nuanced or you have a large list of examples.\n",
    "\n",
    "Check out different types of example selectors [here](https://python.langchain.com/docs/modules/model_io/prompts/example_selectors/)\n",
    "\n",
    "If you want an overview on why examples are important (prompt engineering), check out [this video](https://www.youtube.com/watch?v=dOxUroR57xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "543af2c5aea86b73",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama3\")\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Example Input: {input}\\nExample Output: {output}\",\n",
    ")\n",
    "\n",
    "# Examples of locations that nouns are found\n",
    "examples = [\n",
    "    {\"input\": \"pirate\", \"output\": \"ship\"},\n",
    "    {\"input\": \"pilot\", \"output\": \"plane\"},\n",
    "    {\"input\": \"driver\", \"output\": \"car\"},\n",
    "    {\"input\": \"tree\", \"output\": \"ground\"},\n",
    "    {\"input\": \"bird\", \"output\": \"nest\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bade78f0e38228d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# SemanticSimilarityExampleSelector will select examples that are similar to your input by semantic meaning\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # This is the list of examples available to select from.\n",
    "    examples, \n",
    "    \n",
    "    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    OllamaEmbeddings(model='llama3'), \n",
    "    \n",
    "    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    FAISS, \n",
    "    \n",
    "    # This is the number of examples to produce.\n",
    "    k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31409f6b10a01cbd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# FewShotPromptTemplate -\n",
    "similar_prompt = FewShotPromptTemplate(\n",
    "    # The object that will help select examples\n",
    "    example_selector=example_selector,\n",
    "    \n",
    "    # Your prompt\n",
    "    example_prompt=example_prompt,\n",
    "    \n",
    "    # Customizations that will be added to the top and bottom of your prompt\n",
    "    prefix=\"Give the location an item is usually found in\",\n",
    "    suffix=\"Input: {noun}\\nOutput:\",\n",
    "    \n",
    "    # What inputs your prompt will receive\n",
    "    input_variables=[\"noun\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fef2a9f419caa9e4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the location an item is usually found in\n",
      "\n",
      "Example Input: driver\n",
      "Example Output: car\n",
      "\n",
      "Example Input: tree\n",
      "Example Output: ground\n",
      "\n",
      "Input: student\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "# Select a noun!\n",
    "# my_noun = \"plant\"\n",
    "my_noun = \"student\"\n",
    "\n",
    "print(similar_prompt.format(noun=my_noun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d8af6caea4ebc44",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[llm:Ollama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Give the location an item is usually found in\\n\\nExample Input: driver\\nExample Output: car\\n\\nExample Input: tree\\nExample Output: ground\\n\\nInput: student\\nOutput:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[llm:Ollama] [341ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"I think I can help with that!\\n\\nThe answer would be \\\"desk\\\".\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-07-15T13:36:14.498904Z\",\n",
      "          \"response\": \"\",\n",
      "          \"done\": true,\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"context\": [\n",
      "            128006,\n",
      "            882,\n",
      "            128007,\n",
      "            271,\n",
      "            36227,\n",
      "            279,\n",
      "            3813,\n",
      "            459,\n",
      "            1537,\n",
      "            374,\n",
      "            6118,\n",
      "            1766,\n",
      "            304,\n",
      "            271,\n",
      "            13617,\n",
      "            5688,\n",
      "            25,\n",
      "            5696,\n",
      "            198,\n",
      "            13617,\n",
      "            9442,\n",
      "            25,\n",
      "            1841,\n",
      "            271,\n",
      "            13617,\n",
      "            5688,\n",
      "            25,\n",
      "            5021,\n",
      "            198,\n",
      "            13617,\n",
      "            9442,\n",
      "            25,\n",
      "            5015,\n",
      "            271,\n",
      "            2566,\n",
      "            25,\n",
      "            5575,\n",
      "            198,\n",
      "            5207,\n",
      "            25,\n",
      "            128009,\n",
      "            128006,\n",
      "            78191,\n",
      "            128007,\n",
      "            271,\n",
      "            40,\n",
      "            1781,\n",
      "            358,\n",
      "            649,\n",
      "            1520,\n",
      "            449,\n",
      "            430,\n",
      "            2268,\n",
      "            791,\n",
      "            4320,\n",
      "            1053,\n",
      "            387,\n",
      "            330,\n",
      "            51169,\n",
      "            3343\n",
      "          ],\n",
      "          \"total_duration\": 334950167,\n",
      "          \"load_duration\": 16041834,\n",
      "          \"prompt_eval_count\": 46,\n",
      "          \"prompt_eval_duration\": 19901000,\n",
      "          \"eval_count\": 16,\n",
      "          \"eval_duration\": 297766000\n",
      "        },\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I think I can help with that!\\n\\nThe answer would be \"desk\".'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(similar_prompt.format(noun=my_noun))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0a0f52037738d4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### **Output Parsers Method 1: Prompt Instructions & String Parsing**\n",
    "A helpful way to format the output of a model. Usually used for structured output. LangChain has a bunch more output parsers listed on their [documentation](https://python.langchain.com/docs/modules/model_io/output_parsers).\n",
    "\n",
    "Two big concepts:\n",
    "\n",
    "**1. Format Instructions** - A autogenerated prompt that tells the LLM how to format it's response based off your desired result\n",
    "\n",
    "**2. Parser** - A method which will extract your model's text output into a desired structure (usually json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "821268bd8e574835",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain import PromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89509f14dcdb359b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# How you would like your response structured. This is basically a fancy prompt template\n",
    "llm = Ollama(model=\"llama3\")\n",
    "\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"bad_string\", description=\"This a poorly formatted user input string\"),\n",
    "    ResponseSchema(name=\"good_string\", description=\"This is your response, a reformatted response\")\n",
    "]\n",
    "\n",
    "# How you would like to parse your output\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d21f5538f3e1ea70",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": string  // This a poorly formatted user input string\n",
      "\t\"good_string\": string  // This is your response, a reformatted response\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# See the prompt template you created for formatting\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print (format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33fe49bb67634eb2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You will be given a poorly formatted string from a user.\n",
      "Reformat it and make sure all the words are spelled correctly\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": string  // This a poorly formatted user input string\n",
      "\t\"good_string\": string  // This is your response, a reformatted response\n",
      "}\n",
      "```\n",
      "\n",
      "% USER INPUT:\n",
      "welcom to califonya!\n",
      "\n",
      "YOUR RESPONSE:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "You will be given a poorly formatted string from a user.\n",
    "Reformat it and make sure all the words are spelled correctly\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "% USER INPUT:\n",
    "{user_input}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"user_input\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    "    template=template\n",
    ")\n",
    "\n",
    "promptValue = prompt.format(user_input=\"welcom to califonya!\")\n",
    "\n",
    "print(promptValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c76078117134c56d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[llm:Ollama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"You will be given a poorly formatted string from a user.\\nReformat it and make sure all the words are spelled correctly\\n\\nThe output should be a markdown code snippet formatted in the following schema, including the leading and trailing \\\"```json\\\" and \\\"```\\\":\\n\\n```json\\n{\\n\\t\\\"bad_string\\\": string  // This a poorly formatted user input string\\n\\t\\\"good_string\\\": string  // This is your response, a reformatted response\\n}\\n```\\n\\n% USER INPUT:\\nwelcom to califonya!\\n\\nYOUR RESPONSE:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[llm:Ollama] [2.05s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Here's the response:\\n\\n```json\\n{\\n    \\\"bad_string\\\": \\\"welcom to califonya!\\\",\\n    \\\"good_string\\\": \\\"Welcome to California!\\\"\\n}\\n```\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-07-15T13:43:10.116898Z\",\n",
      "          \"response\": \"\",\n",
      "          \"done\": true,\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"context\": [\n",
      "            128006,\n",
      "            882,\n",
      "            128007,\n",
      "            1432,\n",
      "            2675,\n",
      "            690,\n",
      "            387,\n",
      "            2728,\n",
      "            264,\n",
      "            31555,\n",
      "            24001,\n",
      "            925,\n",
      "            505,\n",
      "            264,\n",
      "            1217,\n",
      "            627,\n",
      "            697,\n",
      "            2293,\n",
      "            433,\n",
      "            323,\n",
      "            1304,\n",
      "            2771,\n",
      "            682,\n",
      "            279,\n",
      "            4339,\n",
      "            527,\n",
      "            68918,\n",
      "            12722,\n",
      "            271,\n",
      "            791,\n",
      "            2612,\n",
      "            1288,\n",
      "            387,\n",
      "            264,\n",
      "            51594,\n",
      "            2082,\n",
      "            44165,\n",
      "            24001,\n",
      "            304,\n",
      "            279,\n",
      "            2768,\n",
      "            11036,\n",
      "            11,\n",
      "            2737,\n",
      "            279,\n",
      "            6522,\n",
      "            323,\n",
      "            28848,\n",
      "            330,\n",
      "            74694,\n",
      "            2285,\n",
      "            1,\n",
      "            323,\n",
      "            330,\n",
      "            74694,\n",
      "            52518,\n",
      "            74694,\n",
      "            2285,\n",
      "            198,\n",
      "            517,\n",
      "            197,\n",
      "            1,\n",
      "            14176,\n",
      "            3991,\n",
      "            794,\n",
      "            925,\n",
      "            220,\n",
      "            443,\n",
      "            1115,\n",
      "            264,\n",
      "            31555,\n",
      "            24001,\n",
      "            1217,\n",
      "            1988,\n",
      "            925,\n",
      "            198,\n",
      "            197,\n",
      "            1,\n",
      "            19045,\n",
      "            3991,\n",
      "            794,\n",
      "            925,\n",
      "            220,\n",
      "            443,\n",
      "            1115,\n",
      "            374,\n",
      "            701,\n",
      "            2077,\n",
      "            11,\n",
      "            264,\n",
      "            15180,\n",
      "            12400,\n",
      "            2077,\n",
      "            198,\n",
      "            534,\n",
      "            14196,\n",
      "            19884,\n",
      "            4,\n",
      "            14194,\n",
      "            27241,\n",
      "            512,\n",
      "            90563,\n",
      "            884,\n",
      "            311,\n",
      "            1652,\n",
      "            333,\n",
      "            113262,\n",
      "            2268,\n",
      "            73613,\n",
      "            77273,\n",
      "            512,\n",
      "            128009,\n",
      "            128006,\n",
      "            78191,\n",
      "            128007,\n",
      "            271,\n",
      "            8586,\n",
      "            596,\n",
      "            279,\n",
      "            2077,\n",
      "            1473,\n",
      "            74694,\n",
      "            2285,\n",
      "            198,\n",
      "            517,\n",
      "            262,\n",
      "            330,\n",
      "            14176,\n",
      "            3991,\n",
      "            794,\n",
      "            330,\n",
      "            90563,\n",
      "            884,\n",
      "            311,\n",
      "            1652,\n",
      "            333,\n",
      "            113262,\n",
      "            34536,\n",
      "            262,\n",
      "            330,\n",
      "            19045,\n",
      "            3991,\n",
      "            794,\n",
      "            330,\n",
      "            14262,\n",
      "            311,\n",
      "            7188,\n",
      "            25765,\n",
      "            534,\n",
      "            74694\n",
      "          ],\n",
      "          \"total_duration\": 2035795041,\n",
      "          \"load_duration\": 1066310041,\n",
      "          \"prompt_eval_count\": 117,\n",
      "          \"prompt_eval_duration\": 300200000,\n",
      "          \"eval_count\": 35,\n",
      "          \"eval_duration\": 666762000\n",
      "        },\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Here\\'s the response:\\n\\n```json\\n{\\n    \"bad_string\": \"welcom to califonya!\",\\n    \"good_string\": \"Welcome to California!\"\\n}\\n```'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_output = llm.invoke(promptValue)\n",
    "llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "560b0d25aeca6e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bad_string': 'welcom to califonya!', 'good_string': 'Welcome to California!'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.parse(llm_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484fb89a6b32f526",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### **Output Parsers Method 2: OpenAI Fuctions**\n",
    "When OpenAI released function calling, the game changed. This is recommended method when starting out.\n",
    "\n",
    "They trained models specifically for outputing structured data. It became super easy to specify a Pydantic schema and get a structured output.\n",
    "\n",
    "There are many ways to define your schema, I prefer using Pydantic Models because of how organized they are. Feel free to reference OpenAI's [documention](https://platform.openai.com/docs/guides/gpt/function-calling) for other methods.\n",
    "\n",
    "In order to use this method you'll need to use a model that supports [function calling](https://openai.com/blog/function-calling-and-other-api-updates#:~:text=Developers%20can%20now%20describe%20functions%20to%20gpt%2D4%2D0613%20and%20gpt%2D3.5%2Dturbo%2D0613%2C). I'll use `gpt4-0613`\n",
    "\n",
    "**Example 1: Simple**\n",
    "\n",
    "Let's get started by defining a simple model for us to extract from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b928d9599055e71",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from typing import Optional\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Identifying information about a person.\"\"\"\n",
    "\n",
    "    name: str = Field(..., description=\"The person's name\")\n",
    "    age: int = Field(..., description=\"The person's age\")\n",
    "    fav_food: Optional[str] = Field(None, description=\"The person's favorite food\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77a6c8ceb9402f4e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['user_input'] partial_variables={'format_instructions': 'The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"bad_string\": string  // This a poorly formatted user input string\\n\\t\"good_string\": string  // This is your response, a reformatted response\\n}\\n```'} template='\\nYou will be given a poorly formatted string from a user.\\nReformat it and make sure all the words are spelled correctly\\n\\n{format_instructions}\\n\\n% USER INPUT:\\n{user_input}\\n\\nYOUR RESPONSE:\\n'\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"user_input\": \"Sally is 13, Joey just turned 12 and loves spinach. Caroline is 10 years older than Sally.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LLMChain > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: \\nYou will be given a poorly formatted string from a user.\\nReformat it and make sure all the words are spelled correctly\\n\\nThe output should be a markdown code snippet formatted in the following schema, including the leading and trailing \\\"```json\\\" and \\\"```\\\":\\n\\n```json\\n{\\n\\t\\\"bad_string\\\": string  // This a poorly formatted user input string\\n\\t\\\"good_string\\\": string  // This is your response, a reformatted response\\n}\\n```\\n\\n% USER INPUT:\\nSally is 13, Joey just turned 12 and loves spinach. Caroline is 10 years older than Sally.\\n\\nYOUR RESPONSE:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LLMChain > llm:ChatOpenAI] [4.24s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"function_call\": {\n",
      "                \"arguments\": \"{\\n  \\\"output\\\": {\\n    \\\"name\\\": \\\"Sally, Joey, Caroline\\\",\\n    \\\"age\\\": 13,\\n    \\\"fav_food\\\": \\\"spinach\\\"\\n  }\\n}\",\n",
      "                \"name\": \"_OutputFormatter\"\n",
      "              }\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 37,\n",
      "                \"prompt_tokens\": 211,\n",
      "                \"total_tokens\": 248\n",
      "              },\n",
      "              \"model_name\": \"gpt-4-0613\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-e1c2321c-4a88-484d-b8af-58323d49fc78-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 211,\n",
      "              \"output_tokens\": 37,\n",
      "              \"total_tokens\": 248\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 37,\n",
      "      \"prompt_tokens\": 211,\n",
      "      \"total_tokens\": 248\n",
      "    },\n",
      "    \"model_name\": \"gpt-4-0613\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LLMChain] [4.24s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Person(name='Sally, Joey, Caroline', age=13, fav_food='spinach')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.openai_functions import create_structured_output_chain\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4-0613', openai_api_key=openai_api_key)\n",
    "\n",
    "chain = create_structured_output_chain(Person, llm, prompt)\n",
    "print(prompt)\n",
    "chain.run(\n",
    "    \"Sally is 13, Joey just turned 12 and loves spinach. Caroline is 10 years older than Sally.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999fcd796b99da00",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Notice how we only have data on one person from that list? That is because we didn't specify we wanted multiple. Let's change our schema to specify that we want a list of people if possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0825ea80f773731",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "class People(BaseModel):\n",
    "    \"\"\"Identifying information about all people in a text.\"\"\"\n",
    "\n",
    "    people: Sequence[Person] = Field(..., description=\"The people in the text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e5f39b579bec1e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "chain = create_structured_output_chain(People, llm, prompt)\n",
    "chain.run(\n",
    "    \"Sally is 13, Joey just turned 12 and loves spinach. Caroline is 10 years older than Sally.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeb706600deace5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Workshop exercise:\n",
    "\n",
    "**Exercise 1:**\n",
    "1. Create a schema for a `Car` with the following fields:\n",
    "    - make: str\n",
    "    - model: str\n",
    "    - year: int\n",
    "    - color: str\n",
    "    - price: float\n",
    "2. create a prompt to extract information about a car from a string\n",
    "3. Run the prompt and parse the output\n",
    "4. Print the output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
