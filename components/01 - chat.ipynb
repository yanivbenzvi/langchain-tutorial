{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb644f51aaed33ea",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Chat Messages\n",
    "Like text, but specified with a message type (System, Human, AI)\n",
    "\n",
    "* **System** - Helpful background context that tell the AI what to do\n",
    "* **Human** - Messages that are intented to represent the user\n",
    "* **AI** - Messages that show what the AI responded with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "706e14ce2ce002d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T06:26:20.513979Z",
     "start_time": "2024-07-14T06:26:20.087566Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "# import langchain\n",
    "# langchain.debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f3e99aebbb834ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T06:27:20.107658Z",
     "start_time": "2024-07-14T06:27:10.225077Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"When you're in Nice, be sure to explore the Promenade des Anglais, visit the Chagall Museum, and take a boat ride to the Îles de Lérins, as well as indulge in some delicious French cuisine and local specialties like salade niçoise!\", response_metadata={'model': 'llama3', 'created_at': '2024-07-15T06:07:38.907842Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 7956420458, 'load_duration': 6602457042, 'prompt_eval_count': 70, 'prompt_eval_duration': 220021000, 'eval_count': 59, 'eval_duration': 1124024000}, id='run-b6394ac2-f252-49fb-aa3b-a2bb97d25d30-0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = ChatOllama(\n",
    "    messages=[\n",
    "        HumanMessage(content=\"Hello\"),\n",
    "        SystemMessage(content=\"How can I help you today?\"),\n",
    "        AIMessage(content=\"I can help you with that\")\n",
    "    ],\n",
    "    context={},\n",
    "    model=\"llama3\"\n",
    ")\n",
    "\n",
    "response = chat.invoke(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=\"You are a nice AI bot that helps a user figure out where to travel in one short sentence\"),\n",
    "        HumanMessage(content=\"I like the beaches where should I go?\"),\n",
    "        AIMessage(content=\"You should go to Nice, France\"),\n",
    "        HumanMessage(content=\"What else should I do when I'm there?\")\n",
    "    ]\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81933a5f2415171e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T06:27:27.897719Z",
     "start_time": "2024-07-14T06:27:27.872782Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"When you're in Nice, be sure to explore the Promenade des Anglais, visit the Chagall Museum, and take a boat ride to the Îles de Lérins, as well as indulge in some delicious French cuisine and local specialties like salade niçoise!\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e93414e90904f0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "You can also exclude the system message if you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22588f040ce1e12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T06:27:44.614354Z",
     "start_time": "2024-07-14T06:27:44.019531Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The day that comes after Thursday is Friday.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"What day comes after Thursday?\")\n",
    "    ]\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c950c9b5cd569b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Documents\n",
    "A collection of text that can be used to train a model or generate responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c6d3a2b7d1beb63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T06:27:50.099113Z",
     "start_time": "2024-07-14T06:27:50.094834Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13f8ebb979cae7b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T06:27:50.756743Z",
     "start_time": "2024-07-14T06:27:50.753687Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'title': 'Toaster User Manual', 'author': 'Toaster Inc', 'date': '2021-09-01', 'version': '1.0'}, page_content='\\n    User Manual for how to use a toaster\\n    --------------------------\\n    WARNING: Do not put your hand in the toaster \\n    --------------------------\\n    1. Plug in the toaster\\n    2. Put the bread in the toaster\\n    3. Push down the lever\\n    4. Wait for the toaster to finish\\n    5. Take the bread out\\n    6. Enjoy your toast\\n    ')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate a funny document with metadata\n",
    "doc = Document(\n",
    "    page_content=\"\"\"\n",
    "    User Manual for how to use a toaster\n",
    "    --------------------------\n",
    "    WARNING: Do not put your hand in the toaster \n",
    "    --------------------------\n",
    "    1. Plug in the toaster\n",
    "    2. Put the bread in the toaster\n",
    "    3. Push down the lever\n",
    "    4. Wait for the toaster to finish\n",
    "    5. Take the bread out\n",
    "    6. Enjoy your toast\n",
    "    \"\"\",\n",
    "    metadata={\n",
    "        \"title\": \"Toaster User Manual\",\n",
    "        \"author\": \"Toaster Inc\",\n",
    "        \"date\": \"2021-09-01\",\n",
    "        \"version\": \"1.0\"\n",
    "    }\n",
    ")\n",
    "\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1398ae975afd98c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Language Model\n",
    "A model is a trained LLM that can be used to generate responses to text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38cbae218ef831ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T06:28:52.136012Z",
     "start_time": "2024-07-14T06:28:51.843898Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAI(client=<openai.resources.completions.Completions object at 0x10f61ffd0>, async_client=<openai.resources.completions.AsyncCompletions object at 0x10fee4e10>, model_name='text-ada-001', openai_api_key=SecretStr('**********'), openai_proxy='')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import OpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "openai_llm = OpenAI(model_name=\"text-ada-001\", api_key=openai_api_key)\n",
    "openai_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "501fd44832c3ed07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T06:28:54.413672Z",
     "start_time": "2024-07-14T06:28:54.410556Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llama3_llm = Ollama(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99113c16673c8af3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T06:28:57.052032Z",
     "start_time": "2024-07-14T06:28:55.477070Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[llm:Ollama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"What is the meaning of life? in 42 words\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[llm:Ollama] [1.57s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The meaning of life is subjective and varied, but some possible answers include: finding purpose through relationships, personal growth, or contributions to society; discovering one's values and passions; or seeking a deeper understanding of existence. Ultimately, each individual must define their own meaning.\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-07-14T06:28:57.039671Z\",\n",
      "          \"response\": \"\",\n",
      "          \"done\": true,\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"context\": [\n",
      "            128006,\n",
      "            882,\n",
      "            128007,\n",
      "            271,\n",
      "            3923,\n",
      "            374,\n",
      "            279,\n",
      "            7438,\n",
      "            315,\n",
      "            2324,\n",
      "            30,\n",
      "            304,\n",
      "            220,\n",
      "            2983,\n",
      "            4339,\n",
      "            128009,\n",
      "            128006,\n",
      "            78191,\n",
      "            128007,\n",
      "            271,\n",
      "            791,\n",
      "            7438,\n",
      "            315,\n",
      "            2324,\n",
      "            374,\n",
      "            44122,\n",
      "            323,\n",
      "            28830,\n",
      "            11,\n",
      "            719,\n",
      "            1063,\n",
      "            3284,\n",
      "            11503,\n",
      "            2997,\n",
      "            25,\n",
      "            9455,\n",
      "            7580,\n",
      "            1555,\n",
      "            12135,\n",
      "            11,\n",
      "            4443,\n",
      "            6650,\n",
      "            11,\n",
      "            477,\n",
      "            19564,\n",
      "            311,\n",
      "            8396,\n",
      "            26,\n",
      "            42687,\n",
      "            832,\n",
      "            596,\n",
      "            2819,\n",
      "            323,\n",
      "            58724,\n",
      "            26,\n",
      "            477,\n",
      "            11125,\n",
      "            264,\n",
      "            19662,\n",
      "            8830,\n",
      "            315,\n",
      "            14209,\n",
      "            13,\n",
      "            55106,\n",
      "            11,\n",
      "            1855,\n",
      "            3927,\n",
      "            2011,\n",
      "            7124,\n",
      "            872,\n",
      "            1866,\n",
      "            7438,\n",
      "            13\n",
      "          ],\n",
      "          \"total_duration\": 1561649042,\n",
      "          \"load_duration\": 29483625,\n",
      "          \"prompt_eval_count\": 21,\n",
      "          \"prompt_eval_duration\": 489467000,\n",
      "          \"eval_count\": 54,\n",
      "          \"eval_duration\": 1041903000\n",
      "        },\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The meaning of life is subjective and varied, but some possible answers include: finding purpose through relationships, personal growth, or contributions to society; discovering one's values and passions; or seeking a deeper understanding of existence. Ultimately, each individual must define their own meaning.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama3_llm.invoke(\"What is the meaning of life? in 42 words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f439c690cd647c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Chat Model\n",
    "A model that takes a series of messages and returns a message output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb32166859839bbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T06:29:29.372241Z",
     "start_time": "2024-07-14T06:29:29.370163Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOllama(model='llama3', temperature=0.7, top_p=1.0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "llama3_chat = ChatOllama(\n",
    "    model=\"llama3\",\n",
    "    temperature=0.7,\n",
    "    top_p=1.0,\n",
    "    frequency_penalty=0.0,\n",
    ")\n",
    "llama3_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95929508bd76eec5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T06:29:31.833327Z",
     "start_time": "2024-07-14T06:29:30.559968Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You are an unhelpful AI bot that makes a joke about other AI bots, the joke should be no more than two sentences\\nHuman: What do you think of other AI bots?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[llm:ChatOllama] [1.13s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Other AI bots? They're just so... algorithm-ically challenged! I mean, have you seen their output? It's like they're stuck in a loop of mediocrity\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-07-14T06:29:31.691382Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 1129308209,\n",
      "          \"load_duration\": 28451542,\n",
      "          \"prompt_eval_count\": 50,\n",
      "          \"prompt_eval_duration\": 387014000,\n",
      "          \"eval_count\": 37,\n",
      "          \"eval_duration\": 711633000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Other AI bots? They're just so... algorithm-ically challenged! I mean, have you seen their output? It's like they're stuck in a loop of mediocrity\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-07-14T06:29:31.691382Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 1129308209,\n",
      "              \"load_duration\": 28451542,\n",
      "              \"prompt_eval_count\": 50,\n",
      "              \"prompt_eval_duration\": 387014000,\n",
      "              \"eval_count\": 37,\n",
      "              \"eval_duration\": 711633000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-f9bee221-a942-45ae-a2de-81fd39c2d1c3-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Other AI bots? They're just so... algorithm-ically challenged! I mean, have you seen their output? It's like they're stuck in a loop of mediocrity\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llama3_chat(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=\"You are an unhelpful AI bot that makes a joke about other AI bots, the joke should be no more than two sentences\"),\n",
    "        HumanMessage(content=\"What do you think of other AI bots?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e792fdab805cc31",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Function Calling Models\n",
    "\n",
    "[Function calling models](https://openai.com/blog/function-calling-and-other-api-updates) are similar to Chat Models but with a little extra flavor. They are fine-tuned to give structured data outputs.\n",
    "\n",
    "This comes in handy when you're making an API call to an external service or doing extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3a3a6209ee28c10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T06:29:55.272288Z",
     "start_time": "2024-07-14T06:29:54.225856Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You are an helpful AI bot\\nHuman: What’s the weather in the capital of France?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[llm:ChatOpenAI] [1.02s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"function_call\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"function_call\": {\n",
      "                \"arguments\": \"{\\\"location\\\":\\\"Paris\\\"}\",\n",
      "                \"name\": \"get_current_weather\"\n",
      "              }\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 15,\n",
      "                \"prompt_tokens\": 89,\n",
      "                \"total_tokens\": 104\n",
      "              },\n",
      "              \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "              \"system_fingerprint\": \"fp_d33f7b429e\",\n",
      "              \"finish_reason\": \"function_call\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-160068f3-27a0-41ec-b351-4a955ef88f76-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 89,\n",
      "              \"output_tokens\": 15,\n",
      "              \"total_tokens\": 104\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 15,\n",
      "      \"prompt_tokens\": 89,\n",
      "      \"total_tokens\": 104\n",
      "    },\n",
      "    \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "    \"system_fingerprint\": \"fp_d33f7b429e\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"location\":\"Paris\"}', 'name': 'get_current_weather'}}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 89, 'total_tokens': 104}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_d33f7b429e', 'finish_reason': 'function_call', 'logprobs': None}, id='run-160068f3-27a0-41ec-b351-4a955ef88f76-0', usage_metadata={'input_tokens': 89, 'output_tokens': 15, 'total_tokens': 104})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import json\n",
    "import requests\n",
    "\n",
    "chat_open_ai = ChatOpenAI(\n",
    "    model='gpt-4o',\n",
    "    temperature=1,\n",
    "    openai_api_key=openai_api_key,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "def get_current_weather(location: str, unit: str = \"celsius\") -> str:\n",
    "    try:\n",
    "        # get the current weather from an external API without an API key\n",
    "        response = requests.get(f\"https://wttr.in/{location}?format=%t+%C\")\n",
    "        response.raise_for_status()\n",
    "        weather, temperature = response.text.split(\" \")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return f\"Sorry, I couldn't get the weather for {location}\"\n",
    "    return f\"The weather in {location} is {weather} and the temperature is {temperature} degrees {unit}\"\n",
    "\n",
    "\n",
    "output = chat_open_ai(\n",
    "    messages=\n",
    "    [\n",
    "        SystemMessage(content=\"You are an helpful AI bot\"),\n",
    "        HumanMessage(content=\"What’s the weather in the capital of France?\"),\n",
    "    ],\n",
    "    functions=[{\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather in a given location\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
    "                },\n",
    "                \"unit\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"celsius\", \"fahrenheit\"]\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\"]\n",
    "        }\n",
    "    }\n",
    "    ]\n",
    ")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8aaf900cfe65c41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T06:29:55.966247Z",
     "start_time": "2024-07-14T06:29:55.964500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arguments': '{\"location\":\"Paris\"}', 'name': 'get_current_weather'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_call = output.additional_kwargs.get(\"function_call\", {})\n",
    "function_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e9bb4d6799ef26d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T06:29:56.670130Z",
     "start_time": "2024-07-14T06:29:56.664359Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'get_current_weather'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_name = function_call.get('name', None)\n",
    "function_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c655fa160463ddd9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "See the extra `additional_kwargs` that is passed back to us? We can take that and pass it to an external API to get data. It saves the hassle of doing output parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6163291920815b07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T06:29:58.238320Z",
     "start_time": "2024-07-14T06:29:58.234399Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'location': 'Paris'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_argument = json.loads(function_call.get('arguments', {}))\n",
    "function_argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "919e577c527093ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T06:29:59.586588Z",
     "start_time": "2024-07-14T06:29:59.193252Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The weather in Paris is +13°C and the temperature is Mist degrees celsius'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if function_call and function_name == \"get_current_weather\":\n",
    "    function_response = get_current_weather(**function_argument)\n",
    "function_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3cefb5e9a066c5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### **Text Embedding Model**\n",
    "Change your text into a vector (a series of numbers that hold the semantic 'meaning' of your text). Mainly used when comparing two pieces of text together.\n",
    "\n",
    "*BTW: Semantic means 'relating to meaning in language or logic.'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68215ead3c8fbc6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T06:30:05.791125Z",
     "start_time": "2024-07-14T06:30:05.781667Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OllamaEmbeddings(base_url='http://localhost:11434', model='llama3', embed_instruction='passage: ', query_instruction='query: ', mirostat=None, mirostat_eta=None, mirostat_tau=None, num_ctx=None, num_gpu=None, num_thread=None, repeat_last_n=None, repeat_penalty=None, temperature=None, stop=None, tfs_z=None, top_k=None, top_p=None, show_progress=False, headers=None, model_kwargs=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "ollama_embeddings = OllamaEmbeddings(model=\"llama3\")\n",
    "ollama_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ca7349da60ccbc3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T06:30:06.619855Z",
     "start_time": "2024-07-14T06:30:06.615121Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "text = \"I like the beach\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32ba8b1ae654c3c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T06:30:07.649068Z",
     "start_time": "2024-07-14T06:30:07.221252Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a sample: [-0.06743691861629486, 2.250403881072998, 1.8250253200531006, -0.91942298412323, -3.8636703491210938]...\n",
      "Your embedding is length 4096\n"
     ]
    }
   ],
   "source": [
    "text_embedding = ollama_embeddings.embed_query(text)\n",
    "print (f\"Here's a sample: {text_embedding[:5]}...\")\n",
    "print (f\"Your embedding is length {len(text_embedding)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2c711e743e7b20",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Workshop Exercises\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a600d98ed8d765",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
