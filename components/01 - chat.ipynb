{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb644f51aaed33ea",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Chat Messages\n",
    "Like text, but specified with a message type (System, Human, AI)\n",
    "\n",
    "* **System** - Helpful background context that tell the AI what to do\n",
    "* **Human** - Messages that are intented to represent the user\n",
    "* **AI** - Messages that show what the AI responded with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "706e14ce2ce002d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T06:26:20.513979Z",
     "start_time": "2024-07-14T06:26:20.087566Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "# import langchain\n",
    "# langchain.debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f3e99aebbb834ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T06:27:20.107658Z",
     "start_time": "2024-07-14T06:27:10.225077Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You are a nice AI bot that helps a user figure out where to travel in one short sentence\\nHuman: I like the beaches where should I go?\\nAI: You should go to Nice, France\\nHuman: What else should I do when I'm there?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[llm:ChatOllama] [1.32s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"When you're in Nice, be sure to visit the Promenade des Anglais for stunning views of the Mediterranean, explore the historic Old Town and its famous Cours Saleya Market, and take a cable car ride up to Castle Hill for panoramic views of the city!\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-07-15T12:43:24.064529Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 1312142417,\n",
      "          \"load_duration\": 46950334,\n",
      "          \"prompt_eval_count\": 70,\n",
      "          \"prompt_eval_duration\": 189923000,\n",
      "          \"eval_count\": 56,\n",
      "          \"eval_duration\": 1072906000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"When you're in Nice, be sure to visit the Promenade des Anglais for stunning views of the Mediterranean, explore the historic Old Town and its famous Cours Saleya Market, and take a cable car ride up to Castle Hill for panoramic views of the city!\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-07-15T12:43:24.064529Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 1312142417,\n",
      "              \"load_duration\": 46950334,\n",
      "              \"prompt_eval_count\": 70,\n",
      "              \"prompt_eval_duration\": 189923000,\n",
      "              \"eval_count\": 56,\n",
      "              \"eval_duration\": 1072906000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-498b6dc0-0844-487d-aa75-2d81dfb38763-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"When you're in Nice, be sure to visit the Promenade des Anglais for stunning views of the Mediterranean, explore the historic Old Town and its famous Cours Saleya Market, and take a cable car ride up to Castle Hill for panoramic views of the city!\", response_metadata={'model': 'llama3', 'created_at': '2024-07-15T12:43:24.064529Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 1312142417, 'load_duration': 46950334, 'prompt_eval_count': 70, 'prompt_eval_duration': 189923000, 'eval_count': 56, 'eval_duration': 1072906000}, id='run-498b6dc0-0844-487d-aa75-2d81dfb38763-0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = ChatOllama(\n",
    "    messages=[\n",
    "        HumanMessage(content=\"Hello\"),\n",
    "        SystemMessage(content=\"How can I help you today?\"),\n",
    "        AIMessage(content=\"I can help you with that\")\n",
    "    ],\n",
    "    context={},\n",
    "    model=\"llama3\"\n",
    ")\n",
    "\n",
    "response = chat.invoke(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=\"You are a nice AI bot that helps a user figure out where to travel in one short sentence\"),\n",
    "        HumanMessage(content=\"I like the beaches where should I go?\"),\n",
    "        AIMessage(content=\"You should go to Nice, France\"),\n",
    "        HumanMessage(content=\"What else should I do when I'm there?\")\n",
    "    ]\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81933a5f2415171e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T06:27:27.897719Z",
     "start_time": "2024-07-14T06:27:27.872782Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"When you're in Nice, be sure to visit the Promenade des Anglais for stunning views of the Mediterranean, explore the historic Old Town and its famous Cours Saleya Market, and take a cable car ride up to Castle Hill for panoramic views of the city!\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e93414e90904f0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "You can also exclude the system message if you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22588f040ce1e12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T06:27:44.614354Z",
     "start_time": "2024-07-14T06:27:44.019531Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: What day comes after Thursday?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[llm:ChatOllama] [626ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The day that comes after Thursday is Friday.\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-07-15T12:43:41.086754Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 612553875,\n",
      "          \"load_duration\": 45558500,\n",
      "          \"prompt_eval_count\": 16,\n",
      "          \"prompt_eval_duration\": 387080000,\n",
      "          \"eval_count\": 10,\n",
      "          \"eval_duration\": 177311000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The day that comes after Thursday is Friday.\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-07-15T12:43:41.086754Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 612553875,\n",
      "              \"load_duration\": 45558500,\n",
      "              \"prompt_eval_count\": 16,\n",
      "              \"prompt_eval_duration\": 387080000,\n",
      "              \"eval_count\": 10,\n",
      "              \"eval_duration\": 177311000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-a00809e6-d34c-41e0-8b95-0924eb1a3dd8-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The day that comes after Thursday is Friday.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"What day comes after Thursday?\")\n",
    "    ]\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c950c9b5cd569b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Documents\n",
    "A collection of text that can be used to train a model or generate responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c6d3a2b7d1beb63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T06:27:50.099113Z",
     "start_time": "2024-07-14T06:27:50.094834Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13f8ebb979cae7b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T06:27:50.756743Z",
     "start_time": "2024-07-14T06:27:50.753687Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'title': 'Toaster User Manual', 'author': 'Toaster Inc', 'date': '2021-09-01', 'version': '1.0'}, page_content='\\n    User Manual for how to use a toaster\\n    --------------------------\\n    WARNING: Do not put your hand in the toaster \\n    --------------------------\\n    1. Plug in the toaster\\n    2. Put the bread in the toaster\\n    3. Push down the lever\\n    4. Wait for the toaster to finish\\n    5. Take the bread out\\n    6. Enjoy your toast\\n    ')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate a funny document with metadata\n",
    "doc = Document(\n",
    "    page_content=\"\"\"\n",
    "    User Manual for how to use a toaster\n",
    "    --------------------------\n",
    "    WARNING: Do not put your hand in the toaster \n",
    "    --------------------------\n",
    "    1. Plug in the toaster\n",
    "    2. Put the bread in the toaster\n",
    "    3. Push down the lever\n",
    "    4. Wait for the toaster to finish\n",
    "    5. Take the bread out\n",
    "    6. Enjoy your toast\n",
    "    \"\"\",\n",
    "    metadata={\n",
    "        \"title\": \"Toaster User Manual\",\n",
    "        \"author\": \"Toaster Inc\",\n",
    "        \"date\": \"2021-09-01\",\n",
    "        \"version\": \"1.0\"\n",
    "    }\n",
    ")\n",
    "\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1398ae975afd98c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Language Model\n",
    "A model is a trained LLM that can be used to generate responses to text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38cbae218ef831ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T06:28:52.136012Z",
     "start_time": "2024-07-14T06:28:51.843898Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAI(client=<openai.resources.completions.Completions object at 0x10c15b390>, async_client=<openai.resources.completions.AsyncCompletions object at 0x10c1527d0>, model_name='text-ada-001', openai_api_key=SecretStr('**********'), openai_proxy='')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import OpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "openai_llm = OpenAI(model_name=\"text-ada-001\", api_key=openai_api_key)\n",
    "openai_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "501fd44832c3ed07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T06:28:54.413672Z",
     "start_time": "2024-07-14T06:28:54.410556Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llama3_llm = Ollama(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99113c16673c8af3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T06:28:57.052032Z",
     "start_time": "2024-07-14T06:28:55.477070Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[llm:Ollama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"What is the meaning of life? in 42 words\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[llm:Ollama] [1.70s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The meaning of life is a profound and subjective question. Some believe it's to find purpose, happiness, or fulfillment through relationships, personal growth, and contributions to society. Others see it as an existential puzzle, searching for answers in philosophy, science, or spirituality. Ultimately, the answer lies within each individual's unique journey.\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-07-15T12:48:25.335723Z\",\n",
      "          \"response\": \"\",\n",
      "          \"done\": true,\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"context\": [\n",
      "            128006,\n",
      "            882,\n",
      "            128007,\n",
      "            271,\n",
      "            3923,\n",
      "            374,\n",
      "            279,\n",
      "            7438,\n",
      "            315,\n",
      "            2324,\n",
      "            30,\n",
      "            304,\n",
      "            220,\n",
      "            2983,\n",
      "            4339,\n",
      "            128009,\n",
      "            128006,\n",
      "            78191,\n",
      "            128007,\n",
      "            271,\n",
      "            791,\n",
      "            7438,\n",
      "            315,\n",
      "            2324,\n",
      "            374,\n",
      "            264,\n",
      "            28254,\n",
      "            323,\n",
      "            44122,\n",
      "            3488,\n",
      "            13,\n",
      "            4427,\n",
      "            4510,\n",
      "            433,\n",
      "            596,\n",
      "            311,\n",
      "            1505,\n",
      "            7580,\n",
      "            11,\n",
      "            23871,\n",
      "            11,\n",
      "            477,\n",
      "            57383,\n",
      "            1555,\n",
      "            12135,\n",
      "            11,\n",
      "            4443,\n",
      "            6650,\n",
      "            11,\n",
      "            323,\n",
      "            19564,\n",
      "            311,\n",
      "            8396,\n",
      "            13,\n",
      "            26080,\n",
      "            1518,\n",
      "            433,\n",
      "            439,\n",
      "            459,\n",
      "            67739,\n",
      "            25649,\n",
      "            11,\n",
      "            15389,\n",
      "            369,\n",
      "            11503,\n",
      "            304,\n",
      "            19675,\n",
      "            11,\n",
      "            8198,\n",
      "            11,\n",
      "            477,\n",
      "            75456,\n",
      "            13,\n",
      "            55106,\n",
      "            11,\n",
      "            279,\n",
      "            4320,\n",
      "            15812,\n",
      "            2949,\n",
      "            1855,\n",
      "            3927,\n",
      "            596,\n",
      "            5016,\n",
      "            11879,\n",
      "            13\n",
      "          ],\n",
      "          \"total_duration\": 1685289375,\n",
      "          \"load_duration\": 36596083,\n",
      "          \"prompt_eval_count\": 21,\n",
      "          \"prompt_eval_duration\": 392036000,\n",
      "          \"eval_count\": 66,\n",
      "          \"eval_duration\": 1255437000\n",
      "        },\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The meaning of life is a profound and subjective question. Some believe it's to find purpose, happiness, or fulfillment through relationships, personal growth, and contributions to society. Others see it as an existential puzzle, searching for answers in philosophy, science, or spirituality. Ultimately, the answer lies within each individual's unique journey.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama3_llm.invoke(\"What is the meaning of life? in 42 words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f439c690cd647c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Chat Model\n",
    "A model that takes a series of messages and returns a message output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb32166859839bbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T06:29:29.372241Z",
     "start_time": "2024-07-14T06:29:29.370163Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOllama(model='llama3', temperature=0.7, top_p=1.0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "llama3_chat = ChatOllama(\n",
    "    model=\"llama3\",\n",
    "    temperature=0.7,\n",
    "    top_p=1.0,\n",
    "    frequency_penalty=0.0,\n",
    ")\n",
    "llama3_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95929508bd76eec5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T06:29:31.833327Z",
     "start_time": "2024-07-14T06:29:30.559968Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yanivbenzvi/develop/tikal/langchain-tutorial/.venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You are an unhelpful AI bot that makes a joke about other AI bots, the joke should be no more than two sentences\\nHuman: What do you think of other AI bots?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[llm:ChatOllama] [1.18s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Other AI bots? Ha! They're just a bunch of \\\"byte-sized\\\" wannabes trying to \\\"reboot\\\" their lives with some AI-ternative humor. (get it?)\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3\",\n",
      "          \"created_at\": \"2024-07-15T12:49:03.533017Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 1172623750,\n",
      "          \"load_duration\": 48296917,\n",
      "          \"prompt_eval_count\": 50,\n",
      "          \"prompt_eval_duration\": 361466000,\n",
      "          \"eval_count\": 40,\n",
      "          \"eval_duration\": 760393000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Other AI bots? Ha! They're just a bunch of \\\"byte-sized\\\" wannabes trying to \\\"reboot\\\" their lives with some AI-ternative humor. (get it?)\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3\",\n",
      "              \"created_at\": \"2024-07-15T12:49:03.533017Z\",\n",
      "              \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"\"\n",
      "              },\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"done\": true,\n",
      "              \"total_duration\": 1172623750,\n",
      "              \"load_duration\": 48296917,\n",
      "              \"prompt_eval_count\": 50,\n",
      "              \"prompt_eval_duration\": 361466000,\n",
      "              \"eval_count\": 40,\n",
      "              \"eval_duration\": 760393000\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-15e450ea-437a-4b3c-b270-b4b730c0c97b-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Other AI bots? Ha! They\\'re just a bunch of \"byte-sized\" wannabes trying to \"reboot\" their lives with some AI-ternative humor. (get it?)'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llama3_chat(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=\"You are an unhelpful AI bot that makes a joke about other AI bots, the joke should be no more than two sentences\"),\n",
    "        HumanMessage(content=\"What do you think of other AI bots?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e792fdab805cc31",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Function Calling Models\n",
    "\n",
    "[Function calling models](https://openai.com/blog/function-calling-and-other-api-updates) are similar to Chat Models but with a little extra flavor. They are fine-tuned to give structured data outputs.\n",
    "\n",
    "This comes in handy when you're making an API call to an external service or doing extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3a3a6209ee28c10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T06:29:55.272288Z",
     "start_time": "2024-07-14T06:29:54.225856Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You are an helpful AI bot\\nHuman: What’s the weather in the capital of France?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[llm:ChatOpenAI] [1.57s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"function_call\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"function_call\": {\n",
      "                \"arguments\": \"{\\\"location\\\":\\\"Paris, France\\\"}\",\n",
      "                \"name\": \"get_current_weather\"\n",
      "              }\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 17,\n",
      "                \"prompt_tokens\": 89,\n",
      "                \"total_tokens\": 106\n",
      "              },\n",
      "              \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "              \"system_fingerprint\": \"fp_298125635f\",\n",
      "              \"finish_reason\": \"function_call\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-81df8418-f705-4414-8546-75db19d6f16d-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 89,\n",
      "              \"output_tokens\": 17,\n",
      "              \"total_tokens\": 106\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 17,\n",
      "      \"prompt_tokens\": 89,\n",
      "      \"total_tokens\": 106\n",
      "    },\n",
      "    \"model_name\": \"gpt-4o-2024-05-13\",\n",
      "    \"system_fingerprint\": \"fp_298125635f\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"location\":\"Paris, France\"}', 'name': 'get_current_weather'}}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 89, 'total_tokens': 106}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_298125635f', 'finish_reason': 'function_call', 'logprobs': None}, id='run-81df8418-f705-4414-8546-75db19d6f16d-0', usage_metadata={'input_tokens': 89, 'output_tokens': 17, 'total_tokens': 106})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import json\n",
    "import requests\n",
    "\n",
    "chat_open_ai = ChatOpenAI(\n",
    "    model='gpt-4o',\n",
    "    temperature=1,\n",
    "    openai_api_key=openai_api_key,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "def get_current_weather(location: str, unit: str = \"celsius\") -> str:\n",
    "    try:\n",
    "        # get the current weather from an external API without an API key\n",
    "        response = requests.get(f\"https://wttr.in/{location}?format=%t+%C\")\n",
    "        response.raise_for_status()\n",
    "        weather, temperature = response.text.split(\" \")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return f\"Sorry, I couldn't get the weather for {location}\"\n",
    "    return f\"The weather in {location} is {weather} and the temperature is {temperature} degrees {unit}\"\n",
    "\n",
    "\n",
    "output = chat_open_ai(\n",
    "    messages=\n",
    "    [\n",
    "        SystemMessage(content=\"You are an helpful AI bot\"),\n",
    "        HumanMessage(content=\"What’s the weather in the capital of France?\"),\n",
    "    ],\n",
    "    functions=[{\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather in a given location\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
    "                },\n",
    "                \"unit\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"celsius\", \"fahrenheit\"]\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\"]\n",
    "        }\n",
    "    }\n",
    "    ]\n",
    ")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8aaf900cfe65c41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T06:29:55.966247Z",
     "start_time": "2024-07-14T06:29:55.964500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arguments': '{\"location\":\"Paris, France\"}', 'name': 'get_current_weather'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_call = output.additional_kwargs.get(\"function_call\", {})\n",
    "function_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e9bb4d6799ef26d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T06:29:56.670130Z",
     "start_time": "2024-07-14T06:29:56.664359Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'get_current_weather'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_name = function_call.get('name', None)\n",
    "function_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c655fa160463ddd9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "See the extra `additional_kwargs` that is passed back to us? We can take that and pass it to an external API to get data. It saves the hassle of doing output parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6163291920815b07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T06:29:58.238320Z",
     "start_time": "2024-07-14T06:29:58.234399Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'location': 'Paris, France'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_argument = json.loads(function_call.get('arguments', {}))\n",
    "function_argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "919e577c527093ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T06:29:59.586588Z",
     "start_time": "2024-07-14T06:29:59.193252Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The weather in Paris, France is +23°C and the temperature is Sunny degrees celsius'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if function_call and function_name == \"get_current_weather\":\n",
    "    function_response = get_current_weather(**function_argument)\n",
    "function_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3cefb5e9a066c5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### **Text Embedding Model**\n",
    "Change your text into a vector (a series of numbers that hold the semantic 'meaning' of your text). Mainly used when comparing two pieces of text together.\n",
    "\n",
    "*BTW: Semantic means 'relating to meaning in language or logic.'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68215ead3c8fbc6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T06:30:05.791125Z",
     "start_time": "2024-07-14T06:30:05.781667Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OllamaEmbeddings(base_url='http://localhost:11434', model='llama3', embed_instruction='passage: ', query_instruction='query: ', mirostat=None, mirostat_eta=None, mirostat_tau=None, num_ctx=None, num_gpu=None, num_thread=None, repeat_last_n=None, repeat_penalty=None, temperature=None, stop=None, tfs_z=None, top_k=None, top_p=None, show_progress=False, headers=None, model_kwargs=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "ollama_embeddings = OllamaEmbeddings(model=\"llama3\")\n",
    "ollama_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ca7349da60ccbc3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T06:30:06.619855Z",
     "start_time": "2024-07-14T06:30:06.615121Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "text = \"I like the beach\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "32ba8b1ae654c3c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T06:30:07.649068Z",
     "start_time": "2024-07-14T06:30:07.221252Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a sample: [-0.06754937767982483, 2.2502830028533936, 1.8254789113998413, -0.9197373390197754, -3.8638994693756104]...\n",
      "Your embedding is length 4096\n"
     ]
    }
   ],
   "source": [
    "text_embedding = ollama_embeddings.embed_query(text)\n",
    "print (f\"Here's a sample: {text_embedding[:5]}...\")\n",
    "print (f\"Your embedding is length {len(text_embedding)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a600d98ed8d765",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
